{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emo-DB Emotion Recognition Model\n",
    "Authors: Sylas Chacko, Ashley Chen, Omari Motta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import librosa \n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "import sklearn.neural_network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in feature extractions\n",
    "\n",
    "df_Mel = pd.read_csv(r'C:\\Users\\sylas\\OneDrive\\emo-db-project\\working\\Mel_Spec_features.csv')\n",
    "df_Chr = pd.read_csv(r'C:\\Users\\sylas\\OneDrive\\emo-db-project\\working\\Chroma_STFT_features.csv')\n",
    "df_MFCC = pd.read_csv(r'C:\\Users\\sylas\\OneDrive\\emo-db-project\\working\\MFCCs_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Extract features and labels\n",
    "df = pd.concat([df_MFCC, df_Mel.drop('Emotion', axis='columns'), df_Chr.drop('Emotion', axis='columns')], axis=1)\n",
    "\n",
    "# Create a new column 'Section' that indexes each group of 87 rows\n",
    "df['Index'] = df.index // 87  # Integer division of the index by 87\n",
    "\n",
    "# Function to split DataFrame into chunks\n",
    "def split_dataframe(df, chunk_size):\n",
    "    return [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Split DataFrame into chunks of 87 rows each\n",
    "chunks = split_dataframe(df, 87)\n",
    "\n",
    "\n",
    "# Shuffle each chunk\n",
    "shuffled_chunks = [chunk.sample(frac=1).reset_index(drop=True) for chunk in chunks]\n",
    "\n",
    "# Concatenate the shuffled chunks back together\n",
    "shuffled_df = pd.concat(shuffled_chunks).reset_index(drop=True)\n",
    "shuffled_index = np.random.permutation(df.index)\n",
    "\n",
    "df_shuffled = df.iloc[shuffled_index].reset_index(drop=True)\n",
    "\n",
    "# Calculate the split index\n",
    "train_size = int(0.8 * len(df_shuffled))\n",
    "test_size = len(df_shuffled) - train_size\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df = df_shuffled.iloc[:train_size]\n",
    "test_df = df_shuffled.iloc[train_size:]\n",
    "\n",
    "y_train, y_test = np.array(train_df['Emotion'].values), np.array(test_df['Emotion'].values)\n",
    "\n",
    "X1_train, X1_test= np.array(train_df.iloc[:,1:14].values), np.array(test_df.iloc[:,1:14].values)\n",
    "\n",
    "X2_train, X2_test = np.array(train_df.iloc[:,14:142].values), np.array(test_df.iloc[:,14:142].values)\n",
    "\n",
    "X3_train, X3_test = np.array(train_df.iloc[:,142:154].values), np.array(test_df.iloc[:,142:154].values)\n",
    "\n",
    "X_train = np.array(train_df.iloc[:,1:154].values)\n",
    "X_test = np.array(test_df.iloc[:,1:154].values)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding) TRAIN\n",
    "\n",
    "# Label encoding using pandas' factorize method\n",
    "labels_train, unique = pd.factorize(y_train)\n",
    "labels_test = pd.Series(y_test).map(lambda x: np.where(unique == x)[0][0]).values\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]  # Using np.eye to create a one-hot encoded matrix\n",
    "\n",
    "# Get number of classes from unique labels\n",
    "num_classes = len(unique)\n",
    "\n",
    "# One-hot encoding for training data\n",
    "y_train_onehot = one_hot_encode(labels_train, num_classes)\n",
    "\n",
    "# One-hot encoding for testing data\n",
    "y_test_onehot = one_hot_encode(labels_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_onehot, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Pass\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Model, Loss Function, and Optimizer\n",
    "\n",
    "input_size = 153  # Number of features\n",
    "hidden_size = 256  # Number of neurons in the hidden layers (this can be tuned)\n",
    "num_classes = num_classes  # Number of emotion classes\n",
    "\n",
    "model = EmotionClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropy is more appropriate for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.8371\n",
      "Epoch [2/50], Loss: 1.8230\n",
      "Epoch [3/50], Loss: 1.7991\n",
      "Epoch [4/50], Loss: 1.7992\n",
      "Epoch [5/50], Loss: 1.8108\n",
      "Epoch [6/50], Loss: 1.8231\n",
      "Epoch [7/50], Loss: 1.8057\n",
      "Epoch [8/50], Loss: 1.7947\n",
      "Epoch [9/50], Loss: 1.8045\n",
      "Epoch [10/50], Loss: 1.8180\n",
      "Epoch [11/50], Loss: 1.8052\n",
      "Epoch [12/50], Loss: 1.7958\n",
      "Epoch [13/50], Loss: 1.8066\n",
      "Epoch [14/50], Loss: 1.7956\n",
      "Epoch [15/50], Loss: 1.7977\n",
      "Epoch [16/50], Loss: 1.7924\n",
      "Epoch [17/50], Loss: 1.7947\n",
      "Epoch [18/50], Loss: 1.7958\n",
      "Epoch [19/50], Loss: 1.8046\n",
      "Epoch [20/50], Loss: 1.8146\n",
      "Epoch [21/50], Loss: 1.8042\n",
      "Epoch [22/50], Loss: 1.8031\n",
      "Epoch [23/50], Loss: 1.8178\n",
      "Epoch [24/50], Loss: 1.8314\n",
      "Epoch [25/50], Loss: 1.8360\n",
      "Epoch [26/50], Loss: 1.8319\n",
      "Epoch [27/50], Loss: 1.8162\n",
      "Epoch [28/50], Loss: 1.8036\n",
      "Epoch [29/50], Loss: 1.7949\n",
      "Epoch [30/50], Loss: 1.8032\n",
      "Epoch [31/50], Loss: 1.7983\n",
      "Epoch [32/50], Loss: 1.7960\n",
      "Epoch [33/50], Loss: 1.8155\n",
      "Epoch [34/50], Loss: 1.7920\n",
      "Epoch [35/50], Loss: 1.7945\n",
      "Epoch [36/50], Loss: 1.7936\n",
      "Epoch [37/50], Loss: 1.7944\n",
      "Epoch [38/50], Loss: 1.8001\n",
      "Epoch [39/50], Loss: 1.8163\n",
      "Epoch [40/50], Loss: 1.8188\n",
      "Epoch [41/50], Loss: 1.8130\n",
      "Epoch [42/50], Loss: 1.8000\n",
      "Epoch [43/50], Loss: 1.7956\n",
      "Epoch [44/50], Loss: 1.8069\n",
      "Epoch [45/50], Loss: 1.8066\n",
      "Epoch [46/50], Loss: 1.8101\n",
      "Epoch [47/50], Loss: 1.7919\n",
      "Epoch [48/50], Loss: 1.7943\n",
      "Epoch [49/50], Loss: 1.7974\n",
      "Epoch [50/50], Loss: 1.8001\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "num_epochs = 50  # Set the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 34.53%\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "\n",
    "torch.save(model.state_dict(), 'emotion_classifier_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo-db-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
